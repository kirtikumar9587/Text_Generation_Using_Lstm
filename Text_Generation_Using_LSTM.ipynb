{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["# keras module for building LSTM \n","from keras.preprocessing.sequence import pad_sequences\n","from keras.layers import Embedding, LSTM, Dense, Dropout\n","from keras.preprocessing.text import Tokenizer\n","from keras.callbacks import EarlyStopping\n","from keras.models import Sequential\n","import keras.utils as ku \n","\n","# set seeds for reproducability\n","from tensorflow import set_random_seed\n","from numpy.random import seed\n","set_random_seed(2)\n","seed(1)\n","\n","import pandas as pd\n","import numpy as np\n","import string, os \n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","warnings.simplefilter(action='ignore', category=FutureWarning)"]},{"cell_type":"markdown","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"},"source":["## 2. Load the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b8ef1429-ff19-4a6c-92d7-af8cc61c55f7","_uuid":"87836e3adbe046dd0db62013491ba62bae93b6be","trusted":true},"outputs":[],"source":["curr_dir = '../input/'\n","all_headlines = []\n","for filename in os.listdir(curr_dir):\n","    if 'Articles' in filename:\n","        article_df = pd.read_csv(curr_dir + filename)\n","        all_headlines.extend(list(article_df.headline.values))\n","        break\n","\n","all_headlines = [h for h in all_headlines if h != \"Unknown\"]\n","len(all_headlines)"]},{"cell_type":"markdown","metadata":{"_cell_guid":"9dbd8bc9-fb61-43b9-b0c4-98bd7f3f8150","_uuid":"fda5d4868631d3618d4d9a9a863541b2faf121c0"},"source":["## 3. Dataset preparation\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b8bf84ed-da11-4f89-a584-9dceea677420","_uuid":"2a07365a27a7ba2f92fc9ba4d05d8e6254a68d8c","trusted":true},"outputs":[],"source":["def clean_text(txt):\n","    txt = \"\".join(v for v in txt if v not in string.punctuation).lower()\n","    txt = txt.encode(\"utf8\").decode(\"ascii\",'ignore')\n","    return txt \n","\n","corpus = [clean_text(x) for x in all_headlines]\n","corpus[:10]"]},{"cell_type":"markdown","metadata":{"_cell_guid":"9d83cc08-19ba-4b00-9ca6-dcf5ff39c8af","_uuid":"6fd11859fd71aa5c7ce10bdbbd31c8eb6d1b3118"},"source":["###  Generating Sequence of N-gram Tokens\n"," \n"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"896543c9-7944-4748-b8ef-ef8cbc2a84f0","_uuid":"9129a8b773feb72eff91aa0025157a173d10c625","trusted":true},"outputs":[],"source":["tokenizer = Tokenizer()\n","\n","def get_sequence_of_tokens(corpus):\n","    ## tokenization\n","    tokenizer.fit_on_texts(corpus)\n","    total_words = len(tokenizer.word_index) + 1\n","    \n","    ## convert data to sequence of tokens \n","    input_sequences = []\n","    for line in corpus:\n","        token_list = tokenizer.texts_to_sequences([line])[0]\n","        for i in range(1, len(token_list)):\n","            n_gram_sequence = token_list[:i+1]\n","            input_sequences.append(n_gram_sequence)\n","    return input_sequences, total_words\n","\n","inp_sequences, total_words = get_sequence_of_tokens(corpus)\n","inp_sequences[:10]"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"73254551-40bd-45b1-a7a5-88fe4cbe0b20","_uuid":"ca588b414e70e21bebcead960f6632805d37dd8c","trusted":true},"outputs":[],"source":["def generate_padded_sequences(input_sequences):\n","    max_sequence_len = max([len(x) for x in input_sequences])\n","    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n","    \n","    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n","    label = ku.to_categorical(label, num_classes=total_words)\n","    return predictors, label, max_sequence_len\n","\n","predictors, label, max_sequence_len = generate_padded_sequences(inp_sequences)"]},{"cell_type":"markdown","metadata":{"_cell_guid":"8b5d80ff-54a8-4380-8a3c-149be880551d","_uuid":"8b8a64b96011f427c48d5b0819e3e74af604ce43"},"source":["## 4. LSTMs for Text Generation"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"60d6721e-e40e-4f2b-8f63-c06459d68f26","_uuid":"76ef6d9352002d333a7c75e8aed7ce996015f527","trusted":true},"outputs":[],"source":["def create_model(max_sequence_len, total_words):\n","    input_len = max_sequence_len - 1\n","    model = Sequential()\n","    \n","    # Add Input Embedding Layer\n","    model.add(Embedding(total_words, 10, input_length=input_len))\n","    \n","    # Add Hidden Layer 1 - LSTM Layer\n","    model.add(LSTM(100))\n","    model.add(Dropout(0.1))\n","    \n","    # Add Output Layer\n","    model.add(Dense(total_words, activation='softmax'))\n","\n","    model.compile(loss='categorical_crossentropy', optimizer='adam')\n","    \n","    return model\n","\n","model = create_model(max_sequence_len, total_words)\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"07d5cf03-d171-4993-9f8b-18446649ecb0","_uuid":"156f3303b8120cc6932e6db985cbea4a7ceb08bf","trusted":true},"outputs":[],"source":["model.fit(predictors, label, epochs=100, verbose=5)"]},{"cell_type":"markdown","metadata":{"_cell_guid":"61e99cfe-7395-4d61-8d1a-8539103d3db5","_uuid":"448bf43b123060dfe4e27cb9f12889e4fe0ed2a7"},"source":["## 5. Generating the text \n"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"e71e56543b7065f115a05e3fd062262b3b94ad46","trusted":true},"outputs":[],"source":["def generate_text(seed_text, next_words, model, max_sequence_len):\n","    for _ in range(next_words):\n","        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n","        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n","        predicted = model.predict_classes(token_list, verbose=0)\n","        \n","        output_word = \"\"\n","        for word,index in tokenizer.word_index.items():\n","            if index == predicted:\n","                output_word = word\n","                break\n","        seed_text += \" \"+output_word\n","    return seed_text.title()"]},{"cell_type":"markdown","metadata":{"_cell_guid":"ea0bddb6-acc6-4592-a2e0-ffc4129a582f","_uuid":"c49bf4ea0e54f3145149e164e243d897f545b84c"},"source":["## 6.Results"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e38dd280-093b-4091-b82b-9aa90045b107","_kg_hide-input":true,"_uuid":"a21548224c9e661a29e3d369e348aada0599bdc9","trusted":true},"outputs":[],"source":["print (generate_text(\"united states\", 5, model, max_sequence_len))\n","print (generate_text(\"preident trump\", 4, model, max_sequence_len))\n","print (generate_text(\"donald trump\", 4, model, max_sequence_len))\n","print (generate_text(\"india and china\", 4, model, max_sequence_len))\n","print (generate_text(\"new york\", 4, model, max_sequence_len))\n","print (generate_text(\"science and technology\", 5, model, max_sequence_len))"]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.11 64-bit (microsoft store)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"},"vscode":{"interpreter":{"hash":"486e0d5a79acdbfffd563ee7a67a93a5017bd2a4f66495483a69f0245c8a4a6c"}}},"nbformat":4,"nbformat_minor":1}
